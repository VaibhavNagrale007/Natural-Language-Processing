{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## DS5601 Assignment 1: Text Classification\n",
        "__Name__ : Vaibhav Nagrale \\\\\n",
        "__RollNo__ : 112001046"
      ],
      "metadata": {
        "id": "JzBhNul0fSC_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gzwQoInCJa0G"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "import math\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from numpy import log, dot, exp, shape\n",
        "import copy\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "oBGr8mPdfONZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/insta/data/cleaned_data_150k.csv')\n",
        "\n",
        "def remove_tags(string):\n",
        "    result = re.sub('','',string)          #remove HTML tags\n",
        "    result = re.sub('https://.*','',result)   #remove URLs\n",
        "    result = re.sub(r'[^a-zA-Z0-9\\s]', ' ',result)    #remove non-alphanumeric characters\n",
        "    result = result.lower()\n",
        "    return result\n",
        "\n",
        "data['text']=data['text'].apply(lambda cw : remove_tags(cw))\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "def lemmatize_text(text):\n",
        "    st = \"\"\n",
        "    for w in w_tokenizer.tokenize(text):\n",
        "        st = st + lemmatizer.lemmatize(w) + \" \"\n",
        "    return st\n",
        "data['text'] = data.text.apply(lemmatize_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXfH2xAYKwHT",
        "outputId": "5b87a1bc-d784-419a-b857-57fed18bef5f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Split"
      ],
      "metadata": {
        "id": "h3cZeGFVfJCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = data['text']\n",
        "labels = data['target']\n",
        "\n",
        "# The dataset is then split into 80% train and 20% test parts using train_test_split from sklearn.model_selection.\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "vec = CountVectorizer()\n",
        "X_train_vec = vec.fit_transform(X_train)\n",
        "X_test_vec = vec.transform(X_test)"
      ],
      "metadata": {
        "id": "TVEFMAMeLiav"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (a) Naive Bayes approach"
      ],
      "metadata": {
        "id": "CzRy6QrgfAxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNaiveBayes:\n",
        "    def train(self, X_train, y_train):\n",
        "        self.unique_classes = np.unique(y_train)\n",
        "        self.class_counts = dict(zip(*np.unique(y_train, return_counts=True)))\n",
        "        self.vocab_size = X_train.shape[1]\n",
        "        self.word_counts_by_class = {label: np.zeros(self.vocab_size) for label in self.unique_classes}\n",
        "        self.total_words_by_class = {label: 0 for label in self.unique_classes}\n",
        "\n",
        "        for i in range(X_train.shape[0]):\n",
        "            class_label = y_train.iloc[i]\n",
        "            self.word_counts_by_class[class_label] += X_train[i].toarray()[0]\n",
        "            self.total_words_by_class[class_label] += X_train[i].sum()\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = []\n",
        "        for i in range(X_test.shape[0]):\n",
        "            class_probs = {}\n",
        "            for class_label in self.unique_classes:\n",
        "                prior_prob = np.log(self.class_counts[class_label] / sum(self.class_counts.values()))\n",
        "                likelihood = np.log((self.word_counts_by_class[class_label] + 1) /\n",
        "                                    (self.total_words_by_class[class_label] + self.vocab_size)).dot(X_test[i].toarray()[0])\n",
        "                class_probs[class_label] = prior_prob + likelihood\n",
        "            predictions.append(max(class_probs, key=class_probs.get))\n",
        "        return predictions\n",
        "\n",
        "# Using the MyNaiveBayes class:\n",
        "nb_model = MyNaiveBayes()\n",
        "nb_model.train(X_train_vec, y_train)\n",
        "\n",
        "pred = nb_model.predict(X_test_vec)"
      ],
      "metadata": {
        "id": "3NG4sAbBLjI2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy and print classification report\n",
        "print(\"My Naive Bayes\")\n",
        "accuracy = accuracy_score(y_test, pred)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "report = classification_report(y_test, pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# Sklearn model accuracy and classification report\n",
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(X_train_vec, y_train)\n",
        "naive_bayes_pred = naive_bayes_classifier.predict(X_test_vec)\n",
        "# Calculate accuracy and print classification report\n",
        "print(\"Sklearn Naive Bayes\")\n",
        "naive_bayes_accuracy = accuracy_score(y_test, naive_bayes_pred)\n",
        "print(\"Accuracy: {:.2f}%\".format(naive_bayes_accuracy * 100))\n",
        "\n",
        "naive_bayes_report = classification_report(y_test, naive_bayes_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(naive_bayes_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUSGiSl4LrBn",
        "outputId": "2b0c0035-d8ff-4176-ccd7-418d1a956620"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My Naive Bayes\n",
            "Accuracy: 84.75%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84      9896\n",
            "           1       0.89      0.76      0.82      9949\n",
            "           2       0.89      0.87      0.88     10155\n",
            "\n",
            "    accuracy                           0.85     30000\n",
            "   macro avg       0.85      0.85      0.85     30000\n",
            "weighted avg       0.85      0.85      0.85     30000\n",
            "\n",
            "Sklearn Naive Bayes\n",
            "Accuracy: 82.48%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.84      0.86      9896\n",
            "           1       0.78      0.77      0.78      9949\n",
            "           2       0.82      0.86      0.84     10155\n",
            "\n",
            "    accuracy                           0.82     30000\n",
            "   macro avg       0.83      0.82      0.82     30000\n",
            "weighted avg       0.83      0.82      0.82     30000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Bayes observation\n",
        "My NB and Sklearn NB both have almost same accurracy \\\\\n",
        "My NB takes time to learn but Sklearn NB is fast to learn."
      ],
      "metadata": {
        "id": "6aFxv4T9o3NJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (b) Logistic regression-based approach"
      ],
      "metadata": {
        "id": "i-nSq8r1LwRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "class MyLogisticRegression:\n",
        "    def __init__(self, num_classes, lr=0.01, num_iter=5000, lambda_reg=0.01):\n",
        "        self.num_classes = num_classes\n",
        "        self.lr = lr\n",
        "        self.num_iter = num_iter\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.weights = None\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(scores):\n",
        "        e_scores = np.exp(scores - np.max(scores))\n",
        "        return e_scores / e_scores.sum(axis=1, keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def cross_entropy(pred, targets):\n",
        "        return -np.sum(targets * np.log(pred + 1e-5)) / pred.shape[0]\n",
        "\n",
        "    def train(self, X, y):\n",
        "        m, n = X.shape\n",
        "        self.weights = np.zeros((n, self.num_classes))\n",
        "        targ = np.eye(self.num_classes)[y]\n",
        "\n",
        "        for _ in range(self.num_iter):\n",
        "            scores = X.dot(self.weights)\n",
        "            predictions = self.softmax(scores)\n",
        "            err = predictions - targ\n",
        "            grad = (X.T.dot(err) + self.lambda_reg * self.weights) / m\n",
        "            self.weights -= self.lr * grad\n",
        "            if _ % 500 == 0:\n",
        "                print(f\"{_}/{self.num_iter}: Loss {self.cross_entropy(predictions, targ)}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = X.dot(self.weights)\n",
        "        pred = self.softmax(scores)\n",
        "        return np.argmax(pred, axis=1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    tfidf_vec = TfidfVectorizer(max_features=5000)\n",
        "    X_train_transform = tfidf_vec.fit_transform(X_train)\n",
        "    X_test_transform = tfidf_vec.transform(X_test)\n",
        "\n",
        "    X_train_intercept = hstack((csr_matrix(np.ones((X_train_transform.shape[0], 1)), dtype=float), X_train_transform))\n",
        "    X_test_intercept = hstack((csr_matrix(np.ones((X_test_transform.shape[0], 1)), dtype=float), X_test_transform))\n",
        "\n",
        "    # Using the MyLogisticRegression class:\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    model = MyLogisticRegression(num_classes=num_classes, lr=0.01, num_iter=5000)\n",
        "    model.train(X_train_intercept, y_train)\n",
        "    pred = model.predict(X_test_intercept)\n",
        "\n",
        "    # My model accuracy and classification report\n",
        "    print(\"My Logistic Regression\")\n",
        "    accuracy = accuracy_score(y_test, pred)\n",
        "    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "    report = classification_report(y_test, pred)\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    # Sklearn model accuracy and classification report\n",
        "    sk_lr = LogisticRegression(max_iter=1000)\n",
        "    sk_lr.fit(X_train_transform, y_train)\n",
        "    sklearn_pred = sk_lr.predict(X_test_transform)\n",
        "\n",
        "    # Calculate accuracy and print classification report\n",
        "    print(\"Sklearn Logistic Regression\")\n",
        "    lr_accurracy = accuracy_score(y_test, sklearn_pred)\n",
        "    print(\"Accuracy: {:.2f}%\".format(lr_accurracy * 100))\n",
        "\n",
        "    lr_report = classification_report(y_test, sklearn_pred)\n",
        "    print(\"Classification Report:\")\n",
        "    print(lr_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzWjkI8DY3ui",
        "outputId": "893ee5d8-02fc-4ba8-b0d7-05a42325afba"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/5000: Loss 1.0985822891181014\n",
            "500/5000: Loss 1.0905094545940157\n",
            "1000/5000: Loss 1.0825767129176151\n",
            "1500/5000: Loss 1.07477718586217\n",
            "2000/5000: Loss 1.0671085536985672\n",
            "2500/5000: Loss 1.0595686377652354\n",
            "3000/5000: Loss 1.0521552562703556\n",
            "3500/5000: Loss 1.044866222706988\n",
            "4000/5000: Loss 1.0376993491728757\n",
            "4500/5000: Loss 1.0306524497570049\n",
            "My Logistic Regression\n",
            "Accuracy: 84.75%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84      9896\n",
            "           1       0.89      0.76      0.82      9949\n",
            "           2       0.89      0.87      0.88     10155\n",
            "\n",
            "    accuracy                           0.85     30000\n",
            "   macro avg       0.85      0.85      0.85     30000\n",
            "weighted avg       0.85      0.85      0.85     30000\n",
            "\n",
            "Sklearn Logistic Regression\n",
            "Accuracy: 93.52%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.94      9896\n",
            "           1       0.93      0.91      0.92      9949\n",
            "           2       0.95      0.94      0.95     10155\n",
            "\n",
            "    accuracy                           0.94     30000\n",
            "   macro avg       0.94      0.94      0.93     30000\n",
            "weighted avg       0.94      0.94      0.94     30000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression observation\n",
        "My LR and Sklearn LR both have almost same accurracy \\\\\n",
        "My LR takes time to learn but Sklearn LR is fast to learn."
      ],
      "metadata": {
        "id": "97CTZXAlpX5x"
      }
    }
  ]
}